{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we talked about how we can use RL in a discrete domain. We talked about two different types of RL, that being value learning and Q learning. We also discussed how increasing the resolution of our domain or adding additional sensing leads to a massive increase in the number of values we need to learn. Since this leads to an increase in both training time and number of samples we need to collect, there has to be a better way to handle continuous domains, aka domains with a functionally infinitely small resolution.\n",
    "\n",
    "Thankfully, there is and it is called actor critic! Rather than learning the value of a single state or a single state action pair, what if we learned a functional approximation of the state value? By that, I mean what if we could learn that V(s) = a*s1 + b*s2? That way, we wouldn’t have to have visited the point [0.562,0.621] before to know its value, we could simply plug that point into the equation to find its value. This is the basic idea of the critic portion of an actor critic. However, even if we know the value of a state we still have a problem. What action do we take next to improve our state value? One idea is to sample a number of points near the current state, feed them into our critic, and take the action that leads to the point with the highest value. This works for a simple domain, but fails under a few common circumstances. \n",
    "\n",
    "For one, as the state dimension increases, the number of points we need to sample increases exponentially. For a one dimensional system, we probably only need to sample 2 points. For a 2-d system, we might be able to get away with 72, essentially giving us a resolution of 5 degrees For a 3-d system, we could need over 2,500 points to get that same 5 degree resolution. \n",
    "\n",
    "For two, this assumes that we can fully map an action to the next state value, which could involve including a full kinematic solver into your agent. That takes significant amounts of time to solve and requires far more knowledge of the system than you may be given.\n",
    "\n",
    "So, instead of sampling using the critic, what do we do? We train a second network that learns a functional mapping from the current state to the current action. Essentially, we want something that says if s1 < 0.5 and s2 > 0.3: a = 4 else a = -4. This network is the actor portion of our actor critic. But how do we know that this network will give us the right performance? Well, every time we take an action, we grade our actor’s performance using the critic. This way, as the critic learns which states are more or less valuable, it passes that information to the actor and it learns which actions lead to a better next state.\n",
    "\n",
    "Lets run through an example. The code below establishes our environment. In this case, we are going to do the same 2d move to goal example that we did previously. This time, our position in x and y is represented by two decimals between 0 and 4 and we can move in both x and y simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Field():\n",
    "    def __init__(self,h,w, start='rand', reward='dist', limited=False):\n",
    "        self.height = h\n",
    "        self.width = w\n",
    "        self.turtle_pos = np.array([w/2,h/2])\n",
    "        self.goal_pos = np.random.rand(2)\n",
    "        self.turtle_orientation = 0\n",
    "        self.start = start\n",
    "        self.reward = reward\n",
    "        self.limited = limited\n",
    "        self.prev_vel = [0,0]\n",
    "        \n",
    "    def update_field_polar(self,action):\n",
    "        if self.limited:\n",
    "            self.turtle_orientation += action[1]/5\n",
    "            if self.turtle_orientation > np.pi*2:\n",
    "                self.turtle_orientation -= np.pi*2\n",
    "            elif self.turtle_orientation < 0:\n",
    "                self.turtle_orientation += np.pi*2\n",
    "        else:\n",
    "            self.turtle_orientation = action[1]*np.pi\n",
    "        self.turtle_pos[1] += action[0] * np.sin(self.turtle_orientation)\n",
    "        self.turtle_pos[0] += action[0] * np.cos(self.turtle_orientation)\n",
    "        self.turtle_pos[0] = np.max([0,np.min([self.turtle_pos[0],self.width])])\n",
    "        self.turtle_pos[1] = np.max([0,np.min([self.turtle_pos[1],self.height])])\n",
    "        return self.calc_reward()\n",
    "\n",
    "    def update_field_cartesian(self,action):\n",
    "        if self.limited:\n",
    "            temp_act = []\n",
    "            temp_act.append(action[0]+self.prev_vel[0]/2)\n",
    "            temp_act.append(action[1]+self.prev_vel[1]/2)\n",
    "            self.turtle_pos[0] += temp_act[0] \n",
    "            self.turtle_pos[1] += temp_act[1]\n",
    "            self.prev_vel = temp_act\n",
    "        else:\n",
    "            self.turtle_pos[0] += action[0] \n",
    "            self.turtle_pos[1] += action[1]\n",
    "        self.turtle_pos[0] = np.max([0,np.min([self.turtle_pos[0],self.width])])\n",
    "        self.turtle_pos[1] = np.max([0,np.min([self.turtle_pos[1],self.height])])\n",
    "        return self.calc_reward()\n",
    "        \n",
    "    def calc_reward(self):\n",
    "        dist = np.linalg.norm(self.turtle_pos-self.goal_pos)\n",
    "        if self.reward == 'sparse':\n",
    "            if dist < (self.height+self.width)/20:\n",
    "                return 10\n",
    "            else:\n",
    "                return -0.3\n",
    "        elif self.reward == 'dist':\n",
    "            return -dist\n",
    "        else:\n",
    "            raise Exception('You need to set the reward type (sparse or dist)')\n",
    "    \n",
    "    def reset(self):\n",
    "        self.turtle_pos = np.array([self.width/2,self.height/2])\n",
    "        if self.start == 'rand':\n",
    "            self.goal_pos = np.random.rand(2)\n",
    "            self.goal_pos[0] = self.goal_pos[0] * self.width\n",
    "            self.goal_pos[1] = self.goal_pos[1] * self.height\n",
    "        elif self.start == 'trivial':\n",
    "            self.goal_pos = np.array([0,0])\n",
    "        elif self.start == 'easy':\n",
    "            self.goal_pos = np.array([0.7,1])\n",
    "        else:\n",
    "            raise Exception('You need to set the start type (trivial, easy or rand)')\n",
    "\n",
    "    \n",
    "    def disp(self):\n",
    "        plt.clf()\n",
    "        plt.scatter(self.turtle_pos[0],self.turtle_pos[1])\n",
    "        plt.scatter(self.goal_pos[0],self.goal_pos[1])\n",
    "        plt.xlim(0,self.width)\n",
    "        plt.ylim(0,self.height)\n",
    "        plt.legend(['Robot', 'Goal'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets actually make the code that handles the actor and critic network. For now, we will just use a couple simple linear layers with relu activation functions. If you want to know more about that or the specific lower level machine learning concepts, take ROB 537 or ask your advisor if low level machine learning is right for you. Side effects may include hair loss, excessive caffeine use and loss of sleep. Here we are also initializing a replay buffer. This is essentially a way to store the state, action, next state and reward for a given timestep. This way, we can learn from a given action more than once by looking at it again when we better understand the value of the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import pi\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def polar2cartesian(polar_tensor):\n",
    "    cartesian_tensor = torch.zeros(polar_tensor.shape).to(device)\n",
    "    if polar_tensor.shape[0] == 2:\n",
    "        if len(polar_tensor.shape) == 1:\n",
    "            r = polar_tensor[0]\n",
    "            theta = polar_tensor[1]\n",
    "            cartesian_tensor[0] = r*torch.cos(theta*pi)\n",
    "            cartesian_tensor[1] = r*torch.sin(theta*pi)\n",
    "        else:\n",
    "            r = polar_tensor[0,:]\n",
    "            theta = polar_tensor[1,:]\n",
    "            cartesian_tensor[0,:] = r*torch.cos(theta*pi)\n",
    "            cartesian_tensor[1,:] = r*torch.sin(theta*pi)\n",
    "    elif polar_tensor.shape[1] == 2:\n",
    "        r = polar_tensor[:,0]\n",
    "        theta = polar_tensor[:,1]\n",
    "        cartesian_tensor[:,0] = r*torch.cos(theta*pi)\n",
    "        cartesian_tensor[:,1] = r*torch.sin(theta*pi)\n",
    "    else:\n",
    "        print('tensor of unexpected shape, returning unchanged')\n",
    "    return cartesian_tensor\n",
    "\n",
    "def cartesian2polar(cartesian_tensor):\n",
    "    polar_tensor = torch.zeros(cartesian_tensor.shape).to(device)\n",
    "    if cartesian_tensor.shape[0] == 2:\n",
    "        if len(cartesian_tensor.shape) ==1:\n",
    "            x = cartesian_tensor[0]\n",
    "            y = cartesian_tensor[1]\n",
    "            polar_tensor[0] = torch.sqrt(x**2+y**2)\n",
    "            polar_tensor[1] = torch.atan2(y,x)/pi\n",
    "        else:\n",
    "            x = cartesian_tensor[0,:]\n",
    "            y = cartesian_tensor[1,:]\n",
    "            polar_tensor[0,:] = torch.sqrt(x**2+y**2)\n",
    "            polar_tensor[1,:] = torch.atan2(y,x)/pi\n",
    "    elif polar_tensor.shape[1] == 2:\n",
    "        x = cartesian_tensor[:,0]\n",
    "        y = cartesian_tensor[:,1]\n",
    "        polar_tensor[:,0] = torch.sqrt(x**2+y**2)\n",
    "        polar_tensor[:,1] = torch.atan2(y,x)/pi\n",
    "    else:\n",
    "        print('tensor of unexpected shape, returning unchanged')\n",
    "\n",
    "    return polar_tensor\n",
    "\n",
    "\n",
    "class critic(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, encoder_type=None):\n",
    "        super(critic, self).__init__()\n",
    "        self.f1 = nn.Linear(input_shape, 10, dtype=torch.float)\n",
    "        self.f1a = nn.Linear(10,10)\n",
    "        self.f2 = nn.Linear(10, output_shape,dtype=torch.float)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.to(self.device)\n",
    "        x = F.relu(self.f1(x))\n",
    "        x = F.relu(self.f1a(x))\n",
    "        x = torch.tanh(self.f2(x)) * 5\n",
    "        return x\n",
    "    \n",
    "class actor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, encoder_type=None):\n",
    "        super(actor, self).__init__()\n",
    "        self.f1 = nn.Linear(input_shape, 10, dtype=torch.float)\n",
    "        self.f1a = nn.Linear(10,10)\n",
    "        self.f2 = nn.Linear(10, output_shape,dtype=torch.float)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.encoder_type = encoder_type\n",
    "        \n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = F.relu(self.f1(x))\n",
    "        x = F.relu(self.f1a(x))\n",
    "        x = torch.tanh(self.f2(x))\n",
    "        if self.encoder_type=='p2c':\n",
    "            x = polar2cartesian(x)\n",
    "        elif self.encoder_type=='c2p':\n",
    "            x = cartesian2polar(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(ReplayMemory.Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets combine the actor and the critic and add functions for running experiments and training our network. Notice that initially we will be adding some random noise to our actions so that we explore the space a little more than we would if we just followed our actor and critic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class TurtleHRL():\n",
    "    def __init__(self,start='rand', reward='dist', control='cartesian', limited=False,encoder_type=None):\n",
    "        \n",
    "        self.BATCH_SIZE = 128\n",
    "        self.GAMMA = 0.999\n",
    "        self.EPS_START = 0.9\n",
    "        self.EPS_END = 0.05\n",
    "        self.EPS_DECAY = 600\n",
    "        self.TARGET_UPDATE = 10\n",
    "        \n",
    "        self.actor_net = actor(6,2,encoder_type).to(device)\n",
    "        self.actor_net.float()\n",
    "        self.actor_target = copy.deepcopy(self.actor_net)\n",
    "        self.critic_net = critic(8,1).to(device)\n",
    "        self.critic_net.eval()\n",
    "        self.critic_target = copy.deepcopy(self.critic_net)\n",
    "        self.old_net = copy.deepcopy(self.critic_net)\n",
    "        self.actor_optimizer = optim.RMSprop(self.actor_net.parameters())\n",
    "        self.critic_optimizer = optim.RMSprop(self.critic_net.parameters())\n",
    "        self.tau = 0.0005\n",
    "\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        \n",
    "        self.start = start\n",
    "        self.reward = reward\n",
    "        self.control = control\n",
    "        self.limited = limited\n",
    "        \n",
    "        self.steps_done = 0\n",
    "\n",
    "        self.current_ang = 0\n",
    "        self.current_pos = [0,0]\n",
    "\n",
    "        self.aloss = np.zeros((10000,))\n",
    "        self.closs = np.zeros((10000,))\n",
    "        self.ep = 0\n",
    "        \n",
    "        self.rewards = []\n",
    "        \n",
    "        self.modified = False # True means we transform from cartesian to polar\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.actor_net(state.float())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                rand_thing =  4*torch.tensor([[random.random()-0.5,random.random()-0.5]], device=device, dtype=torch.float)\n",
    "                thing = (self.actor_net(state.float()) +rand_thing).clamp(-1,1)\n",
    "                return thing        \n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = ReplayMemory.Transition(*zip(*transitions))\n",
    "    \n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=device, dtype=torch.bool)\n",
    "        next_states = [s for s in batch.next_state if s is not None]\n",
    "        next_states = torch.stack(next_states)\n",
    "        states = [s for s in batch.state if s is not None]\n",
    "        states = torch.stack(states)\n",
    "        next_actions = [self.actor_target(s) for s in next_states]\n",
    "        alt_actions = [self.actor_net(s) for s in states]\n",
    "        actions = [s for s in batch.action if s is not None]\n",
    "        non_final_next_states_with_action = [torch.cat((next_states[i].to(device),next_actions[i])) for i in range(len(next_states))]\n",
    "        non_final_next_states_with_action = torch.stack(non_final_next_states_with_action)\n",
    "        state_and_action =  [torch.cat((states[i].to(device),actions[i].to(device))) for i in range(len(states))]\n",
    "        state_and_action = torch.stack(state_and_action)\n",
    "        alt_state_and_action =  [torch.cat((states[i].to(device),alt_actions[i].to(device))) for i in range(len(states))]\n",
    "        alt_state_and_action = torch.stack(alt_state_and_action)\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward).to(device)\n",
    "        state_action_values = self.critic_net(state_and_action)\n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=device)        \n",
    "        next_state_values[non_final_mask] = self.critic_target(non_final_next_states_with_action).detach().reshape(self.BATCH_SIZE)\n",
    "        expected_q_values = (next_state_values * self.GAMMA) + reward_batch.to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        critic_loss = criterion(state_action_values, expected_q_values.unsqueeze(1))\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        for param in self.critic_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.critic_optimizer.step()\n",
    "        actor_loss = -self.critic_net(alt_state_and_action).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        \n",
    "        for param in self.actor_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.actor_optimizer.step()\n",
    "        self.aloss[self.ep] = actor_loss.cpu().detach().numpy()\n",
    "        self.closs[self.ep] = critic_loss.cpu().detach().numpy()\n",
    "        self.ep +=1\n",
    "        if self.ep % 10 == 9:\n",
    "            for param, target_param in zip(self.critic_net.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            for param, target_param in zip(self.actor_net.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        \n",
    "    def collect_data(self,num_episodes,flag=False):\n",
    "        field = Field(4,4, self.start, self.reward, self.limited)\n",
    "        # you get 30 timesteps to get to the desired pose.\n",
    "        for ep in range(num_episodes):\n",
    "            field.reset()\n",
    "            for i in range(30):\n",
    "                state = list(field.turtle_pos) + [np.sin(field.turtle_orientation),np.cos(field.turtle_orientation)] + list(field.goal_pos)\n",
    "                state = torch.from_numpy(np.array(state)).float()\n",
    "                raw_action = self.select_action(state)\n",
    "                raw_action = raw_action.to('cpu')\n",
    "                if len(raw_action) == 1:\n",
    "                    raw_action = raw_action[0]\n",
    "                if self.control == 'cartesian':\n",
    "                    done = field.update_field_cartesian(raw_action.numpy())\n",
    "                elif self.control == 'polar':\n",
    "                    done = field.update_field_polar(raw_action.numpy())\n",
    "                else:\n",
    "                    raise Exception('You need to set the control parameter (cartesian or polar)')\n",
    "                next_state = list(field.turtle_pos) + [np.sin(field.turtle_orientation),np.cos(field.turtle_orientation)]  + list(field.goal_pos)\n",
    "                next_state = torch.from_numpy(np.array(next_state)).float()\n",
    "                if done > -0.2:\n",
    "                    reward = 5\n",
    "                    self.memory.push(state, raw_action, next_state ,torch.tensor([reward],dtype=torch.float))\n",
    "                    break\n",
    "                else:\n",
    "                    reward = done\n",
    "                    self.memory.push(state, raw_action, next_state ,torch.tensor([reward],dtype=torch.float))\n",
    "                if flag:\n",
    "                    print('state: ',state)\n",
    "                    print('action: ',raw_action)\n",
    "                    print('next state: ',next_state)\n",
    "                    print('reward: ', reward)\n",
    "            self.rewards.append(reward)\n",
    "    \n",
    "    def disp_episode(self,action=False):\n",
    "        field = Field(4,4, self.start, self.reward, self.limited)\n",
    "        # you get 30 timesteps to get to the desired pose.\n",
    "        field.reset()\n",
    "        for i in range(30):\n",
    "            display.clear_output(wait=True)\n",
    "            field.disp()\n",
    "            time.sleep(0.5)\n",
    "            state = list(field.turtle_pos) + [np.sin(field.turtle_orientation),np.cos(field.turtle_orientation)] + list(field.goal_pos)\n",
    "            state = torch.from_numpy(np.array(state)).float()\n",
    "            raw_action = self.select_action(state)\n",
    "            raw_action = raw_action.to('cpu')\n",
    "            if len(raw_action) == 1:\n",
    "                raw_action = raw_action[0]\n",
    "            if action:\n",
    "                print('action: ', raw_action)\n",
    "            if self.control == 'cartesian':\n",
    "                done = field.update_field_cartesian(raw_action.numpy())\n",
    "            elif self.control == 'polar':\n",
    "                done = field.update_field_polar(raw_action.numpy())\n",
    "            next_state = list(field.turtle_pos) + [np.sin(field.turtle_orientation),np.cos(field.turtle_orientation)]  + list(field.goal_pos)\n",
    "            next_state = torch.from_numpy(np.array(next_state)).float()\n",
    "            if done > -0.2:\n",
    "                break\n",
    "        display.clear_output(wait=True)\n",
    "        field.disp()\n",
    "        print('Finished Episode')\n",
    "        \n",
    "    def plot_reward(self):\n",
    "        display.clear_output(wait=True)\n",
    "        final_rewards = moving_average(self.rewards,10)\n",
    "        plt.plot(range(len(final_rewards)),final_rewards)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Average Reward (10 episodes)\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything in place, lets run through a few examples. First, lets do the most basic example. The goal will always be in the bottom left corner of the map at 0,0. Furthermore, we consider any point that comes within 0.5 units of 0,0 to be successful. If you run the code below, that is what we will train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('trivial','dist')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "reward_percents = []\n",
    "epochs = []\n",
    "for i in range(200):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 10 == 9:\n",
    "        reward_percents.append(100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        epochs.append(i)\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        plt.plot(epochs,reward_percents)\n",
    "        plt.ylabel('success %')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we get a high percentage of successful trials, lets visualize the performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.disp_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well (as it should). Why is this example so easy, and how can we make it harder?\n",
    "\n",
    "Well first off, we can always move down and left and expect to succeed. It doesn’t matter if we go all the way down then left, or all the way left then down, we will get to the goal. How about we put the goal at 0.7,1 instead? This way, if we always move down and left we may miss the goal, especially if we hit the wall first. Lets run that now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('easy','dist')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(10)\n",
    "reward_percents = []\n",
    "epochs = []\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()    \n",
    "    if i % 10 == 9:\n",
    "        reward_percents.append(100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        epochs.append(i)\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        plt.plot(epochs,reward_percents)\n",
    "        plt.ylabel('success %')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('easy','dist','polar')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(10)\n",
    "reward_percents = []\n",
    "epochs = []\n",
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 10 == 9:\n",
    "        reward_percents.append(100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        epochs.append(i)\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,reward_percents)\n",
    "plt.ylabel('success %')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that this time it takes noticeably longer to train the agent. Keep in mind, this is still an easy task to complete, but it takes a while longer to get done. \n",
    "\n",
    "Lets visualize the agent now that it is fully trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f.disp_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartesian Limited\n",
    "\n",
    "f = TurtleHRL('easy','dist','cartesian',True)\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(10)\n",
    "reward_percents = []\n",
    "epochs = []\n",
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 10 == 9:\n",
    "        reward_percents.append(100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        epochs.append(i)\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,reward_percents)\n",
    "plt.ylabel('success %')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.disp_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polar Limited\n",
    "\n",
    "f = TurtleHRL('easy','dist','polar',True)\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(10)\n",
    "reward_percents = []\n",
    "epochs = []\n",
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 10 == 9:\n",
    "        reward_percents.append(100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        epochs.append(i)\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,reward_percents)\n",
    "plt.ylabel('success %')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.disp_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in theory this should be a polar field, but our control implementation is in cartesian\n",
    "\n",
    "f = TurtleHRL('easy','dist','polar',encoder_type='c2p')\n",
    "f.collect_data(10)\n",
    "reward_percents = []\n",
    "epochs = []\n",
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 10 == 9:\n",
    "        reward_percents.append(100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        epochs.append(i)\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,reward_percents)\n",
    "plt.ylabel('success %')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in theory this should be a cartesian field, but our control implementation is in polar\n",
    "\n",
    "f = TurtleHRL('easy','dist','cartesian',encoder_type='p2c')\n",
    "f.collect_data(10)\n",
    "reward_percents = []\n",
    "epochs = []\n",
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 10 == 9:\n",
    "        reward_percents.append(100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        epochs.append(i)\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,reward_percents)\n",
    "plt.ylabel('success %')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('easy','dist')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(10)\n",
    "reward_percents = []\n",
    "epochs = []\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()    \n",
    "    if i % 10 == 9:\n",
    "        reward_percents.append(100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        epochs.append(i)\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.actor_net.encoder_type='c2p'\n",
    "f.control = 'polar'\n",
    "f.encoder_type='c2p'\n",
    "f.collect_data(100)\n",
    "print('Percent of 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = TurtleHRL('rand','dist')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(10)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(2500):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time getting above 90% takes a long time. We still get there (or close, im not running the code, im just a script so I can't tell if you got unlucky) but it is much more time consuming and there were probably dips in performance from time to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.disp_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try making things even more interesting. This time we will take away our heuristic and try only giving the agent a positive reward if it gets to the goal. If not, it will get a reward of -0.3. This type of sparse reward function is common for tasks where we can't easily construct a heuristic. Again we will start with the trivial example in the bottom corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('trivial','sparse')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(200):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we still do well on this task. Its still a good idea to try the most basic example to make sure the problem is still solveable. Lets move on to the easy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('easy','sparse')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is also still learned. Maybe we didn't need to have the heuristic at all!\n",
    "\n",
    "Try the random start position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('rand','sparse')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(2500):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we gave it 2500 epochs, it doesn't learn how to handle the problem. Why is that?\n",
    "\n",
    "A good way to evaluate this is to look at the critic loss. It starts high, but then drops substantially, only interrupted by small momentary peaks. This is because the critic is assuming all actions are equally bad (or close to it). There are a very small number of successful trials in our dataset, and even for those, the only action with a positive reward is the last one. That means there is a lot of pressure for the network to assume an action will result in a reward of -0.3. As a result, it finds a local minima where it always gives -0.3 and just is wrong every once in a blue moon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Lets try a different control scheme. This time, rather than the action being the change in x and y, we instead make it the velocity in the direction it is facing and the change in angle. Lets try reinstating the heuristic and going through the difficulties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('trivial','dist','polar')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(200):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the trivial problem is still doable. Lets vizualize it for fun though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.disp_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a bit of a less direct path this time because we can only rotate so quickly, but it is still finding the shortest route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('easy','dist','polar')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.disp_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('rand','dist','polar')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(2500):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.disp_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, run the polar coordinates with the sparse reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('trivial','sparse','polar')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(200):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('easy','dist','polar')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(2000):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TurtleHRL('rand','dist','polar')\n",
    "#collect some initial data so that the replay buffer has at least enough data to fill a batch\n",
    "f.collect_data(2)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(2500):\n",
    "    f.collect_data(1)\n",
    "    f.optimize_model()\n",
    "    if i % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print('epoch ', i ,' finished')\n",
    "        print('Percent of past 100 trials that were successful', 100*np.average(np.array(f.rewards[i-99:i+1])>0))\n",
    "        print('average a loss for last 100 trials', np.average(f.aloss[i-99:i+1]))\n",
    "        print('average c loss for last 100 trials', np.average(f.closs[i-99:i+1]))\n",
    "        plt.plot(range(i-1),f.aloss[:i-1])\n",
    "        plt.plot(range(i-1),f.closs[:i-1])\n",
    "        plt.legend(['actor loss', 'critic loss'])\n",
    "        plt.show()\n",
    "end = time.time()\n",
    "print('training took, ', round(end-start,2), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few takaways from this exercise. \n",
    "1. If a task is easy enough, you can throw RL at it in the worst way possible and it will probably succeed. It won't be meaningful but it will work\n",
    "2. If it is not trivial, you will need to be smart about your states, actions and rewards. It was easier for the algorithm to handle cartesian actions rather than polar because each action was independent of the other AND it lined up directly with the reward signal. Also, heuristics that accurately reflect the value of the state are incredibly useful.\n",
    "3. Always try the easiest possible situation and work your way up. If we started with the random goal pose and no heuristic, we wouldn't know if our implementation of the algorithm was wrong or if the task was too complex.\n",
    "4. Sparse rewards can work, but you need to have the right situation. You can't arbitrarily throw RL at any problem that can't be easily measured by using sparse rewards. If you do use sparse rewards, there are many ways to make them more effective (such as expert replay, ranking transition importance, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
