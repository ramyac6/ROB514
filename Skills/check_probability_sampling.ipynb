{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"check_probability_sampling.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is in this file/notebook\n",
    "\n",
    "Slides: https://docs.google.com/presentation/d/1vZC32UCamhyJWJBQIuP5AXKK896mBges_eO2H3Vo3q4/edit?usp=sharing\n",
    "\n",
    "How do you use numpy's stats package to generalize a single random variable for:\n",
    "a) Booleans (T/F)\n",
    "b) Non-gaussian distributions\n",
    "c) Discrete variables (a, b, c)\n",
    "d) Binned \"continuous\" variables - 0.1-0.2, 0.2-0.3, etc\n",
    "\n",
    "Think of these as functions as simulating real-world events - query the sensor for if the door is open (y/n), ask where the robot is (contiuous location OR a grid square in the world), ask which room you're in (discrete variable, kitchen dining room, etc). These are all fancy versions of a coin toss (returns T/F with 50% probability each), a roll of the dice (returns 1..6 with equal probability).\n",
    "\n",
    "ALL of these \"simulate probability\" routines can be implemented using JUST numpy's uniform number generator  (generates a number between 0 and 1 with all values equally likely). The simplest way to think of all of these methods is that you chop up the unit interval 0..1 into the number of possible outcomes, with each bit of the unit interval representing how likely that event is. Then you just generate a number from 0 to 1 and see which bin you fell into.\n",
    "\n",
    "For continuous probability functions, you use uniform twice - once to pick the x value, once to pick the y value. We will not do this here.\n",
    "\n",
    "Why the functions are set up they way they are: You need to input how likely each discrete event is. There's three basic methods for specifying this.\n",
    "1) List each discrete event and how likely it is\n",
    "2) All events are equally likely, just say how many there are (bins) and the mapping between the bins and the 'labels'.Usually the bins represent some spatial variable like location or angle, but could be movement\n",
    "3) There is a function that represents how likely each event is, with the x coordinate representing some continuous variable like distance (think Gaussian error for movement)\n",
    "\n",
    "For each method that you'll implement the above information is passed in using a dictionary. I'm using a dictionary (instead of a class) because it's a bit easier to understand/implement, but the 'right' way to do this is as a class (see the last, optional, problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports you'll need\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean\n",
    "Simplest case - returns True or False, with some probability\n",
    "\n",
    "Since probability of returning False is 1-prob(True), only need to specify one value in info_variable info_variable[\"prob_return_true\"] has the probability of returning True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_boolean_variable(info_variable):\n",
    "    \"\"\" Generate one sample from a boolean variable\n",
    "    @param info_variable contains the probability of the sensor returning True\n",
    "    @returns True or False \"\"\"\n",
    "\n",
    "    # Probabilities have to be between 0 and 1...\n",
    "    if info_variable[\"prob_return_true\"] < 0.0 or info_variable[\"prob_return_true\"] > 1.0:\n",
    "        ValueError(f\"Value {info_variable['prob_return_true']} not between zero and one\")\n",
    "\n",
    "    # First, use random.uniform to generate a number between 0 and one. Note that this is a uniform distribution, not\n",
    "    #  a Gaussian one\n",
    "    zero_to_one = random.uniform()\n",
    "\n",
    "    # See slides - if the random variable is below the probability of returning true, return true. Otherwise, return false\n",
    "    # TODO: Return True or False\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # First, check that you have no obvious syntax errors \n",
    "    boolean_variable = {\"prob_return_true\": 0.7}\n",
    "    ret_val = sample_boolean_variable(boolean_variable)\n",
    "    if ret_val is True or ret_val is False:\n",
    "        print(\"sample_boolean: Passed syntax check\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test_boolean(test_prob_value=0.6, b_print=True):\n",
    "    \"\"\" Check if the sample_boolean is doing the right thing by calling it lots of times\n",
    "    @param test_prob_value - any value between 0.0 and 1.0\n",
    "    @param b_print - whether or not to print out intermediate results\n",
    "    @returns True if sample_boolean_variable is working correctly\"\"\"\n",
    "\n",
    "    if b_print:\n",
    "        print(\"Testing boolean\")\n",
    "    boolean_variable = {\"prob_return_true\": test_prob_value}\n",
    "\n",
    "    count_true = 0\n",
    "    count_false = 0\n",
    "    for _ in range(0, 10000):\n",
    "        if sample_boolean_variable(boolean_variable) == True:\n",
    "            count_true += 1\n",
    "        else:\n",
    "            count_false += 1\n",
    "\n",
    "    perc_true = count_true / (count_true + count_false)\n",
    "    if b_print:\n",
    "        print(f\"Perc true from sampling: {perc_true}, expected {boolean_variable['prob_return_true']}\")\n",
    "    if not np.isclose(perc_true, boolean_variable[\"prob_return_true\"], atol=0.05):\n",
    "        if b_print:\n",
    "            print(\"Failed\")\n",
    "        return False\n",
    "\n",
    "    if b_print:\n",
    "        print(\"Passed\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: This should return true and print out passed. However, sometimes the random number generator will not be\n",
    "#  your friend and it will fail - you're expecting the count to come out around 0.6 +- noise\n",
    "print(f\"Boolean result: {test_boolean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"boolean_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete\n",
    "\n",
    "A list of discrete variables and their corresponding likelihood.\n",
    "\n",
    "Because we need one value for each variable, and a name for each variable, store this as name/probabilty pair. Name is the key, probability is the value\n",
    "\n",
    "If you have forgotten how to do dictionaries, go do the tutorial on dictionaries before attempting this problem\n",
    "\n",
    "Hint: You want the .items() iterator. If you don't know what that is, or don't know what a key, value pair is, go do the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_discrete_variable(info_variable):\n",
    "    \"\"\" Generate one sample from the given discrete variable distribution\n",
    "    Your code should NOT need to know what the actual keys are, how many there are, or what\n",
    "    the actual values are - i.e., your code should NOT include things like\n",
    "             if key == \"True\" or if z < 0.8\n",
    "    @param info_variable contains pairs of values with probabilities. Probabilites should sum to one\n",
    "    @returns one of the discrete values (keys) in the dictionary \"\"\"\n",
    "\n",
    "    # First, I'll do some checks for you\n",
    "    for v in info_variable.values():\n",
    "        # Probabilities have to be between 0 and 1...\n",
    "        if v < 0.0 or v > 1.0:\n",
    "            ValueError(f\"Value {v} not between zero and one\")\n",
    "\n",
    "    # And they have to sum to one\n",
    "    if not np.isclose(sum(info_variable.values()), 1.0):\n",
    "        ValueError(f\"Sum of probabilities should be 1, is {sum(info_variable.values())}\")\n",
    "\n",
    "    # Now, use random to generate a number between 0 and one\n",
    "    zero_to_one = random.uniform()\n",
    "\n",
    "    # See slides - \"stack\" the probabilities - if the value lies in the discrete value's stack, return that one\n",
    "    #  needs a for loop\n",
    "    # You should use a FOR loop. If you're struggling with the FOR loop, try writing this with an if - else if -else if\n",
    "    #  for JUST the test case in the next cell - it won't work for test_discrete\n",
    "    #  \n",
    "    # TODO - return one of the key values in the dictionary. \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax check - does your code run and return a key?\n",
    "\n",
    "check_discrete_tri = {\"red\": 0.2, \"green\": 0.5, \"blue\": 0.3}\n",
    "ret_value = sample_discrete_variable(check_discrete_tri)\n",
    "if ret_value == \"red\" or ret_value == \"green\" or ret_value == \"blue\":\n",
    "    print(\"Discrete: Passed syntax check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_discrete(b_print=True):\n",
    "    if b_print:\n",
    "        print(\"Testing discrete, three cases\")\n",
    "    # The following for loop will loop through each of these in turn. It is NOT doing them all at the\n",
    "    #  same time - the first time through the for loop it will check the boolean case, the second time\n",
    "    #  the red, green, blue, the third time the quad one\n",
    "    check_boolean = {\"True\": 0.6, \"False\": 0.4}\n",
    "    check_discrete_tri = {\"red\": 0.2, \"green\": 0.5, \"blue\": 0.3}\n",
    "    check_discrete_quad = {\"kitchen\": 0.2, \"living room\": 0.3, \"dining room\": 0.3, \"bed room\": 0.2}\n",
    "    for check_variable in [check_boolean, check_discrete_tri, check_discrete_quad]:\n",
    "        # For each discrete variable, set the counts to be zero; save as dictionary (rather than array/list) because\n",
    "        #   the keys are strings\n",
    "        counts = {}\n",
    "        for k in check_variable.keys():\n",
    "            counts[k] = 0\n",
    "\n",
    "        # 'throw the dice' multiple times, and update counts as you go\n",
    "        n_samples = 50000\n",
    "        for _ in range(0, n_samples):\n",
    "            # Which discrete variable?\n",
    "            discrete_value = sample_discrete_variable(check_variable)\n",
    "            # Add one to that discrete variable's count\n",
    "            counts[discrete_value] += 1\n",
    "\n",
    "        # Now compare the percentage values\n",
    "        for k, v in check_variable.items():\n",
    "            perc = counts[k] / n_samples\n",
    "            if b_print:\n",
    "                print(f\"Discrete value: {k}, got: {perc}, expected {v}\")\n",
    "\n",
    "            if not np.isclose(perc, v, atol=0.05):\n",
    "                if b_print:\n",
    "                    print(\"Failed\")\n",
    "                return False\n",
    "    if b_print:\n",
    "        print(\"Passed\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note, this is a little slow\n",
    "print(f\"Discrete result: {test_discrete()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"discrete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bins \n",
    "\n",
    "This is actually a special case of the previous function - just that we don't explicitly label the bins; instead the labels are set to the value at the center of the bin. Rather than specifying unique labels for each bin,\n",
    "just provide the start/stop boundaries and the number of divisions. Assumes all bins are equally likely.\n",
    "\n",
    "Your solution should NOT have a loop in it - you should be able to calculate which bin zero_to_one lies in directly (see np.floor(x))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_bin_variable(info_variable):\n",
    "    \"\"\"Return the center location of the bin the sensor value lies in\n",
    "    @param info_variable - bin start and stop, number of bins\n",
    "    @return The value (center) associated with the bin\"\"\"\n",
    "\n",
    "    zero_to_one = random.uniform()\n",
    "    # TODO:\n",
    "    #  Step 1: Calculate the size of each bin ON THE UNIT INTERVAL\n",
    "    #  Step 2: Use np.floor to find the INDEX of the bin\n",
    "    #  Step 3: Calculate the center of the bin with that index on the (start, stop) interval\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking the syntax of the call\n",
    "check_bins = {\"start\": -2.0, \"stop\": 3.0, \"n bins\": 10}\n",
    "bin_loc = sample_bin_variable(check_bins)\n",
    "if check_bins[\"start\"] < bin_loc < check_bins[\"stop\"]:\n",
    "    print(\"bin sampling: return value is in correct range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_bins(b_print=True):\n",
    "    if b_print:    \n",
    "        print(\"Testing bins\")\n",
    "    # Provide the start and stop values, and the number of bins\n",
    "    check_bins = {\"start\": -2.0, \"stop\": 3.0, \"n bins\": 10}\n",
    "\n",
    "    counts = np.zeros(check_bins[\"n bins\"])\n",
    "\n",
    "    n_samples = 50000\n",
    "    bin_width = (check_bins[\"stop\"] - check_bins[\"start\"]) / check_bins[\"n bins\"]\n",
    "    for _ in range(0, n_samples):\n",
    "        # Which bin location was selected?\n",
    "        bin_loc = sample_bin_variable(check_bins)\n",
    "\n",
    "        # Convert back to the bin id\n",
    "        bin = int(np.floor((bin_loc - check_bins[\"start\"]) / bin_width))\n",
    "        # Add one to that\n",
    "        counts[bin] += 1\n",
    "\n",
    "    # All of the percentage values should be the same\n",
    "    perc_expected = 1.0 / check_bins[\"n bins\"]\n",
    "    for i, count in enumerate(counts):\n",
    "        perc_found = count / n_samples\n",
    "        bin_loc = check_bins[\"start\"] + (i + 0.5) * bin_width\n",
    "        if b_print:\n",
    "            print(f\"Bin loc {bin_loc} perc {perc_found} expected {perc_expected}\")\n",
    "\n",
    "        if not np.isclose(perc_found, perc_expected, atol=0.05):\n",
    "            if b_print:\n",
    "                print(\"Failed\")\n",
    "            return False\n",
    "    if b_print:\n",
    "        print(\"Passed\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Bin result: {test_bins()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"bins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian sampling\n",
    "This is a generic Gaussian noise variable - I'm including it here because you'll need it in subsequent assignments. But it's basically \"store mu and sigma, then use those to generate noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_gaussian_variable(info_variable):\n",
    "    \"\"\"Return a sample from the Gaussian\n",
    "    @param info_variable - mu and sigma\n",
    "    @return A sample from the Gaussian\"\"\"\n",
    "\n",
    "    # Call random.normal here\n",
    "    # TODO: Call random.normal here and return a number\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking syntax of call\n",
    "check_gaussian = {\"mu\": 1.2, \"sigma\": 0.2}\n",
    "sample = sample_gaussian_variable(check_gaussian)\n",
    "print(f\"Sample value should be a number: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_gaussian(b_print=True):\n",
    "    \"\"\"Test the gaussian distribution by seeing if the mean/sd are the same\n",
    "    @param b_print print out test results y/n\n",
    "    \"\"\"\n",
    "    if b_print:\n",
    "        print(\"Testing Gaussian\")\n",
    "    # Provide mu and sigma\n",
    "    check_gaussian = {\"mu\": 1.2, \"sigma\": 0.2}\n",
    "\n",
    "    n_samples = 50000\n",
    "    # This does the for loop \"in one line\" - read this as\n",
    "    #   for _ in range()\n",
    "    #       sample_gaussian...\n",
    "    samples = [sample_gaussian_variable(check_gaussian) for _ in range(0, n_samples)]\n",
    "\n",
    "    # Should get out same mu/sigma\n",
    "    samples_mean = np.mean(samples)\n",
    "    samples_sigma = np.std(samples)\n",
    "\n",
    "    if not np.isclose(samples_mean, check_gaussian[\"mu\"], atol=0.05):\n",
    "        raise ValueError(f\"Failed Gaussian, expected {check_gaussian['mu']}, got {samples_mean}\")\n",
    "\n",
    "    if not np.isclose(samples_sigma, check_gaussian['sigma'], atol=0.05):\n",
    "        raise ValueError(f\"Failed Gaussian, expected {check_gaussian['sigma']}, got {samples_sigma}\\n\")\n",
    "\n",
    "    if b_print:\n",
    "        print(\"Passed\\n\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Gaussian result: {test_gaussian()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Optional: Probability mass function (discrete) \n",
    "\n",
    "This is a more general version of the previous bin variable, with the main difference being that each bin has a different liklihood (as specified by the input function). So it's a combination of the discrete variable (using a running sum to determine which bin you fall in) and the bins (chopping up a continuous variable into bins).\n",
    "\n",
    "Technical note: In theory land, there is a difference between doing this as a continuous function (probability density) versus chopping it up into pieces (probability mass). You can actually do continuous functions, but it's a bit trickier and we don't need it (see for example https://www.comsol.com/blogs/sampling-random-numbers-from-probability-distribution-functions/)\n",
    "\n",
    "For this example we're going to use a class instead of a method because (in order to make it efficient) you want to pre-calculate a running sum from the given probabilities. It would be very expensive to do this every time you asked for a sample, like you did in the discrete problem.\n",
    "\n",
    "This is also a good time to do some fancy numpy array stuff, namely, using \"where\" to find the index (instead of writing your own for loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SampleProbabilityMassFunction:\n",
    "    def __init__(self, in_pdf, x_range=(0.0, 1.0), n_bins=100):\n",
    "        \"\"\" Given a probability mass function, what range of x to use, and the number of samples, create the running\n",
    "        sum/data needed to generate random samples from that pmf\n",
    "        @param in_pdf - the function representing the probability distribution\n",
    "        @param x_range - min and max x values as a tuple\n",
    "        @param n_bins - number of bins \"\"\"\n",
    "\n",
    "        # TODO - Initialize correctly\n",
    "        #  Where the bins start and end\n",
    "        #  The amount of probability to put in each bin\n",
    "        #  The running sum\n",
    "        # Where the center of each bin is (see sample_bin_variable above)\n",
    "        self.bin_centers = np.zeros(n_bins)\n",
    "        \n",
    "        # Create the pmf by evaluating in_pdf at the center of each bin\n",
    "        #   Don't forget to normalize - the sum of self.bin_heights should be 1\n",
    "        self.bin_heights = np.zeros(n_bins)\n",
    "        # Running sum of probabilities - bin_sum[i] = sum(bin_heights[0:i])\n",
    "        #  Note: It's a bit easier to generate_sample if you make this array n_bins+1, with the first value being 0\n",
    "        #   and the last value being 1         \n",
    "        self.bin_sum = np.zeros(n_bins+1)\n",
    "\n",
    "    def generate_sample(self):\n",
    "        \"\"\" Draw one sample from the pmf\n",
    "        Very similar to the discrete example above, for picking which bin, except you've pre-calculated the running sum.\n",
    "        Very similar to bin_sample for returning the bin center, exept you've pre-calculated the bin centers\n",
    "        @return bin center \"\"\"\n",
    "        zero_to_one = random.uniform()\n",
    "\n",
    "        # You want the i where bin_sum[i] <= zero_to_one < bin_sum[i+1]\n",
    "        # Not fancy version: Use a for loop\n",
    "        # Fancy version: Use np.where\n",
    "        # TODO - return correct bin center               \n",
    "        ...\n",
    "\n",
    "    def _generate_counts(self, n_samples):\n",
    "        \"\"\" Generate n samples\n",
    "        @param n_samples - number of samples\n",
    "        @returns a numpy array with the counts for each bin, normalized\"\"\"\n",
    "\n",
    "        # Counts\n",
    "        counts = np.zeros(self.bin_centers.shape[0])\n",
    "\n",
    "        # Make sure to take enough samples for all of the bins...\n",
    "        bin_width = self.bin_centers[1] - self.bin_centers[0]\n",
    "        for _ in range(0, self.bin_centers.shape[0] * 100):\n",
    "            x_value = self.generate_sample()\n",
    "            bin_index = np.ceil((x_value - self.bin_centers[0]) / bin_width)\n",
    "            counts[int(bin_index)] += 1.0\n",
    "\n",
    "        # Normalize\n",
    "        counts = counts / sum(counts)\n",
    "        return counts\n",
    "\n",
    "    def test_self(self, in_pdf):\n",
    "        \"\"\" Check/test function\n",
    "        @param in_pdf - the pdf function used to generate the values\n",
    "        @returns True/False\"\"\"\n",
    "\n",
    "        # Expected probability values\n",
    "        expected_probs = in_pdf(self.bin_centers)\n",
    "        # Normalize\n",
    "        expected_probs /= np.sum(expected_probs)\n",
    "\n",
    "        counts = self._generate_counts(100 * self.bin_centers.shape[0])\n",
    "\n",
    "        for exp, c in zip(expected_probs, counts):\n",
    "            print(f\"pmf perc {c} expected {exp}\")\n",
    "\n",
    "            if np.abs(exp - c) > 0.1:\n",
    "                print(\"Failed\")\n",
    "                return False\n",
    "\n",
    "        print(\"Passed\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pdf(x):\n",
    "    \"\"\" Made-up pdf (a quadratic). Can be anything, as long as it's not negative\n",
    "    @param x\n",
    "    @ return (x+1) * (x+1) + 0.1\"\"\"\n",
    "    return (x+1) ** 2 + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax check\n",
    "x_min = -2.0\n",
    "x_max = 1.0\n",
    "n_bins = 10\n",
    "\n",
    "# Make the class\n",
    "my_sample = SampleProbabilityMassFunction(pdf, (x_min, x_max), n_bins)\n",
    "# Generate a sample\n",
    "ret_value = my_sample.generate_sample()\n",
    "if x_min < ret_value < x_max:\n",
    "    print(\"PMF: Passed syntax check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_pmf(b_do_plot=True):\n",
    "    # Make the class\n",
    "    x_min = -2.0\n",
    "    x_max = 1.0\n",
    "    n_bins = 10\n",
    "    print(\"Sample pmf\")\n",
    "    my_sample = SampleProbabilityMassFunction(pdf, (x_min, x_max), n_bins)\n",
    "\n",
    "    if not b_do_plot:\n",
    "        return my_sample.test_self(pdf)\n",
    "    \n",
    "    print(f\"Passed test: {my_sample.test_self(pdf)}\")\n",
    "    \n",
    "    # Plot the results\n",
    "    _, axs = plt.subplots(1, 2)\n",
    "    xs = np.linspace(x_min, x_max, n_bins * 10)\n",
    "    ys = pdf(xs)\n",
    "    ys = ys / sum(pdf(my_sample.bin_centers))\n",
    "    axs[0].plot(xs, ys, '-k', label=\"pdf\")\n",
    "    axs[0].plot(my_sample.bin_centers, my_sample.bin_heights, 'bX', label=\"pmf\")\n",
    "    axs[0].legend()\n",
    "    axs[0].set_title(\"pdf to pmf\")\n",
    "\n",
    "    # The more samples you take, the more it will look like the pmf\n",
    "    counts = my_sample._generate_counts(1000 * n_bins)\n",
    "    axs[1].plot(xs, ys, '-k', label=\"pdf\")\n",
    "    axs[1].plot(my_sample.bin_centers, counts, 'bX', label=\"pmf samples\")\n",
    "    axs[1].legend()\n",
    "    axs[1].set_title(\"Sampled pmf\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pmf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Hours and collaborators\n",
    "Required for every assignment - fill out before you hand-in.\n",
    "\n",
    "Listing names and websites helps you to document who you worked with and what internet help you received in the case of any plagiarism issues. You should list names of anyone (in class or not) who has substantially helped you with an assignment - or anyone you have *helped*. You do not need to list TAs.\n",
    "\n",
    "Listing hours helps us track if the assignments are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO - set to correct value               \n",
    "# List of names (creates a set)\n",
    "worked_with_names = {\"not filled out\"}\n",
    "# List of URLS (creates a set)\n",
    "websites = {\"not filled out\"}\n",
    "# Approximate number of hours, including lab/in-class time\n",
    "hours = -1.5\n",
    "\n",
    "# for all row, column in all_indices_from_where\n",
    "#.   if this is the column for wrist torque \n",
    "#.      print(f\"Row: {r}, Time step: {c // n_time_steps} Successful y/n: {pick_data[r, -1] == 1}, value: {pick_data[r, c]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"hours_collaborators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Submit through gradescope, probability_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "otter": {
   "tests": {
    "Gaussian": {
     "name": "Gaussian",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(test_gaussian(b_print=False))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "bins": {
     "name": "bins",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(test_bins(b_print=False))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "boolean_sample": {
     "name": "boolean_sample",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(test_boolean(0.3, b_print=False))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "discrete": {
     "name": "discrete",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(test_discrete(b_print=False))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "hours_collaborators": {
     "name": "hours_collaborators",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(not \"not filled out\" in worked_with_names)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(not \"not filled out\" in websites)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(hours > 0)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
